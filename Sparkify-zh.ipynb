{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify 项目 Workspace\n",
    "这个 Workspace 包括一个迷你的子数据集（128MB），是完整数据集（12GB）的一个子集。在将你的项目部署到云上之前，你可以自由使用 Workspace 来创建你的项目或用Spark来探索这个较小数据集。设置 Spark 集群的指南可以在选修 Spark 课程的内容里找到。\n",
    "\n",
    "你可以依照下面的步骤进行项目的数据分析和模型搭建部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import datetime\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,min,max,avg\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "path = \"mini_sparkify_event_data.json\"\n",
    "data = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载和清洗数据\n",
    "在这个 Workspace 中，小数据集的名称是 `mini_sparkify_event_data.json`.加载和清洗数据集，检查是否有无效或缺失数据——例如，没有userid或sessionid的数据。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the counts\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number that session ID is not empty \n",
    "data.filter(data.sessionId == \"\" ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number that userID is not empty \n",
    "data.filter(data.userId == \"\" ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the line that there is no userID\n",
    "data.dropna(how = \"any\",subset=[\"sessionId\",\"userId\"] )\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 探索性数据分析\n",
    "当你使用完整数据集时，通过加载小数据集，在 Spark 中完成基础操作来实现探索性数据分析。在这个 Workspace 中，我们已经提供给你一个你可以探索的小数据集。\n",
    "\n",
    "### 定义客户流失\n",
    "\n",
    "在你完成初步分析之后，创建一列 `Churn` 作为模型的标签。我建议你使用 `Cancellation Confirmation` 事件来定义客户流失，该事件在付费或免费客户身上都有发生。作为一个奖励任务，你也可以深入了解 `Downgrade` 事件。\n",
    "\n",
    "### 探索数据\n",
    "你定义好客户流失后，就可以执行一些探索性数据分析，观察留存用户和流失用户的行为。你可以首先把这两类用户的数据聚合到一起，观察固定时间内某个特定动作出现的次数或者播放音乐的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the column\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many values inside page column\n",
    "data.select(\"page\").drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many users did the confirmed cancellation\n",
    "data.filter(data.page==\"Cancellation Confirmation\").select(\"userId\").dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the time format and add a time column\n",
    "get_time = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "data = data.withColumn(\"time\", get_time(data.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the time column\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Churn column, ture means user did Cancellation Confirmation, otherwise it is false\n",
    "cc_users = data.filter(data.page==\"Cancellation Confirmation\").select(\"userId\").dropDuplicates()\n",
    "cc_users_list = [(row['userId']) for row in cc_users.collect()]\n",
    "data_withchurn = data.withColumn(\"churn\", data.userId.isin(cc_users_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the value of churn column\n",
    "data_withchurn.select([\"userId\",\"churn\"]).where(data.userId == \"125\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis the churn by the level\n",
    "data_level = data_withchurn.filter(data_withchurn.churn == \"true\").select([\"level\",\"churn\"]).groupby(\"level\").count()\n",
    "data_level = data_level.toPandas()\n",
    "data_level.head()\n",
    "\n",
    "plt.pie(x = list(data_level[\"count\"]),labels=list(data_level[\"level\"]),autopct=\"%0.2f%%\")\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis the churn by the gender \n",
    "data_gendor = data_withchurn.dropDuplicates([\"userId\", \"gender\"]).groupby([\"churn\", \"gender\"]).count().sort(\"churn\").toPandas()\n",
    "sns.barplot(x='churn', y='count', hue='gender', data=data_gendor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis the churn by time and date, analysis the data and add 3 columns\n",
    "\n",
    "hour = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).hour)\n",
    "data_withchurn = data_withchurn.withColumn(\"hour\", hour(data_withchurn.ts))\n",
    "\n",
    "weekday = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%w\"))\n",
    "data_withchurn = data_withchurn.withColumn(\"weekday\", weekday(data_withchurn.ts))\n",
    "\n",
    "day = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).day)\n",
    "data_withchurn = data_withchurn.withColumn(\"day\", day(data_withchurn.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the transferred time column\n",
    "data_withchurn.select([\"userId\",\"hour\",\"weekday\",\"day\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_date_page_churn(date,page,churn,color,label):\n",
    "    '''\n",
    "    Define a function to show the plt for date/page/churn\n",
    "    '''\n",
    "    data_pd = data_withchurn.filter(data_withchurn.page == page).groupby(\"churn\", date).count().orderBy(data_withchurn[date].cast(\"float\")).toPandas()\n",
    "    data_pd[data_pd.churn == churn].plot.bar(x = date, y = 'count', color= color, label= label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot_date_page_churn(\"hour\",\"NextSong\",True,\"Green\",\"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot_date_page_churn(\"weekday\",\"Downgrade\",False,\"red\",\"not churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot_date_page_churn(\"weekday\",\"Downgrade\",False,\"yellow\",\"not churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot_date_page_churn(\"day\",\"Cancellation Confirmation\",True,\"green\",\"churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程\n",
    "熟悉了数据之后，就可以构建你认为会对训练模型帮助最大的特征。要处理完整数据集，你可以按照下述步骤：\n",
    "- 写一个脚本来从小数据集中提取你需要的特征\n",
    "- 确保你的脚本可以拓展到大数据集上，使用之前教过的最佳实践原则\n",
    "- 在完整数据集上运行你的脚本，按运行情况调试代码\n",
    "\n",
    "如果是在教室的 workspace，你可以直接用里面提供的小数据集来提取特征。确保当你开始使用 Spark 集群的时候，把上述的成果迁移到大数据集上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#features 1: get user regestion days\n",
    "user_max_ts = data_withchurn.groupby(\"userId\").max(\"ts\").sort(\"userId\")\n",
    "user_reg_ts = data_withchurn.select(\n",
    "    \"userId\", \"registration\").dropDuplicates().sort(\"userId\")\n",
    "user_reg_days = user_reg_ts.join(\n",
    "    user_max_ts, user_reg_ts.userId == user_max_ts.userId).select(\n",
    "        user_reg_ts[\"userId\"],\n",
    "        ((user_max_ts[\"max(ts)\"] - user_reg_ts[\"registration\"]) /\n",
    "         (1000 * 60 * 60 * 24)).alias(\"regDays\"))\n",
    "\n",
    "user_reg_days.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features 2 If user is a paid user or free user\n",
    "#get the user paid /free status with the maximum timestramp \n",
    "user_pf_status = data_withchurn.groupby(\"userId\", \"level\").agg(max(data_withchurn.ts).alias(\"lastTime\")).sort(\"userId\")\n",
    "#get the user's latest timestramp \n",
    "user_lastest_level_time = user_pf_status.groupby(\"userId\").agg(max(user_pf_status.lastTime).alias(\"lastest\"))\n",
    "#Join the 2 tables\n",
    "temp = user_pf_status.join(user_lastest_level_time,[user_pf_status.userId == user_lastest_level_time.userId, user_lastest_level_time.lastest == user_pf_status.lastTime]).select(user_pf_status.userId,user_pf_status.level)\n",
    "#transfer the data, free is 0, paid is 1\n",
    "user_level = temp.replace([\"free\", \"paid\"], [\"0\", \"1\"], \"level\")\n",
    "user_level = user_level.withColumnRenamed(\"userId\", \"user_level_id\")\n",
    "user_level.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features 3 User's gender \n",
    "user_gender = data_withchurn.select(\"userId\", \"gender\").dropDuplicates()\n",
    "user_gender = user_gender.replace([\"M\", \"F\"], [\"0\", \"1\"], \"gender\")\n",
    "user_gender = user_gender.select(\"userId\", \"gender\")\n",
    "user_gender = user_gender.withColumnRenamed(\"userId\", \"user_gender_id\")\n",
    "user_gender.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features 4 How many songs user has been interacted\n",
    "\n",
    "user_song_interact = data_withchurn.select(\"userId\",\"song\").drop_duplicates().groupby(\"userId\").count()\n",
    "user_song_interact = user_song_interact.withColumnRenamed(\"userId\", \"user_song_interact_id\")\n",
    "user_song_interact.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature 5: how many singers has the user heard\n",
    "user_artist_count = data_withchurn.filter(data_withchurn.page==\"NextSong\").select(\"userId\", \"artist\").dropDuplicates().groupby(\"userId\").count()\n",
    "user_artist_count = user_artist_count.withColumnRenamed(\"count\", \"aritstCount\")\n",
    "user_artist_count = user_artist_count.withColumnRenamed(\"userId\", \"user_artist_count_id\")\n",
    "user_artist_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature 6 time indicator per user\n",
    "user_session_time = data_withchurn.groupby(\"userId\", \"sessionId\").agg(((max(data_withchurn.ts)-min(data_withchurn.ts))/(1000*60)).alias(\"sessionTime\"))\n",
    "user_session_time_stat = user_session_time.groupby(\"userId\").agg(avg(user_session_time.sessionTime).alias(\"SessionavgTime\"), min(user_session_time.sessionTime).alias(\"SessionminTime\"), max(user_session_time.sessionTime).alias(\"SessionmaxTime\")).sort(\"userId\")\n",
    "user_session_time_stat = user_session_time_stat.withColumnRenamed(\"userId\", \"user_session_time_stat_id\")\n",
    "user_session_time_stat.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features 7 get number of the songs for each session\n",
    "user_session_songs_count = data_withchurn.filter(data_withchurn.page==\"NextSong\").groupby(\"userId\", \"sessionId\").count()\n",
    "user_session_songs_avg = user_session_songs_count.groupby(\"userId\").agg(avg(\"count\").alias(\"avgSessionSongs\")).sort(\"userId\")\n",
    "user_session_songs_avg = user_session_songs_avg.withColumnRenamed(\"userId\", \"user_session_songs_avg_id\")\n",
    "user_session_songs_avg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all the features to user_data dataframe\n",
    "user_data = user_reg_days.join(user_level,user_reg_days.userId == user_level.user_level_id).drop(\"user_level_id\")\n",
    "user_data = user_data.join(user_gender,user_data.userId == user_gender.user_gender_id).drop(\"user_gender_id\")\n",
    "user_data = user_data.join(user_song_interact,user_data.userId == user_song_interact.user_song_interact_id).drop(\"user_song_interact_id\")\n",
    "user_data = user_data.join(user_artist_count,user_data.userId == user_artist_count.user_artist_count_id).drop(\"user_artist_count_id\")\n",
    "user_data = user_data.join(user_session_time_stat,user_data.userId == user_session_time_stat.user_session_time_stat_id).drop(\"user_session_time_stat_id\")\n",
    "user_data = user_data.join(user_session_songs_avg,user_data.userId == user_session_songs_avg.user_session_songs_avg_id).drop(\"user_session_songs_avg_id\")\n",
    "\n",
    "user_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get user's lable data\n",
    "user_label = data_withchurn.select(\"userId\", \"churn\").dropDuplicates()\n",
    "user_label = user_label.select(\"userId\", user_label.churn.cast(\"int\"))\n",
    "user_label = user_label.withColumnRenamed(\"userId\", \"user_label_id\")\n",
    "user_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Features and Labels Together\n",
    "user_data_final = user_data.join(user_label,user_data.userId == user_label.user_label_id).drop(\"user_label_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show final data\n",
    "user_data_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the data into CSV File\n",
    "path = \"user_data_final.csv\"\n",
    "user_data_final.toPandas().to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模\n",
    "将完整数据集分成训练集、测试集和验证集。测试几种你学过的机器学习方法。评价不同机器学习方法的准确率，根据情况调节参数。根据准确率你挑选出表现最好的那个模型，然后报告在训练集上的结果。因为流失顾客数据集很小，我建议选用 F1 score 作为优化指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"user_data_final.csv\"\n",
    "user_data_final = spark.read.csv(path, header=True)\n",
    "user_data_final.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer all the features to numeric\n",
    "for col in user_data_final.columns[1:]:\n",
    "    col_name = col + \"Num\"\n",
    "    user_data_final = user_data_final.withColumn(col_name, user_data_final[col].cast(\"float\"))\n",
    "    user_data_final = user_data_final.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the features into vector\n",
    "assembler = VectorAssembler(inputCols=user_data_final.columns[1:-1], outputCol=\"NumFeatures\")\n",
    "data = assembler.transform(user_data_final)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"NumFeatures\", outputCol=\"ScaledNumFeatures\", withStd=True)\n",
    "scalerModel = scaler.fit(data)\n",
    "data = scalerModel.transform(data)\n",
    "\n",
    "data = data.select(data.churnNum.alias(\"label\"), data.ScaledNumFeatures.alias(\"features\"))\n",
    "\n",
    "#Split the data into train set and validation set\n",
    "train, validation = data.randomSplit([0.8, 0.2], seed=42)\n",
    "train = train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using LR model\n",
    "lr =  LogisticRegression()\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam,[0.0, 0.1, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.regParam,[0.0, 0.05, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "Model_lr = crossval.fit(train)\n",
    "Model_lr.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user gbt model\n",
    "gbt  = GBTClassifier()\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter,[3, 10, 20]) \\\n",
    "    .addGrid(gbt.maxDepth,[2, 4, 6, 8]) \\\n",
    "    .build()\n",
    "\n",
    "crossval_gbt = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "Model_gbt = crossval_gbt.fit(train)\n",
    "Model_gbt.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Decision Tree Model\n",
    "dt = DecisionTreeClassifier()\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.impurity,['entropy', 'gini']) \\\n",
    "    .addGrid(dt.maxDepth,[2, 3, 4, 5, 6, 7, 8]) \\\n",
    "    .build()\n",
    "\n",
    "crossval_dt = CrossValidator(estimator=dt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "Model_dt = crossval_dt.fit(train)\n",
    "Model_dt.avgMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a fucntion to calculate F1 Score\n",
    "def getF1(Model,validation):\n",
    "    prediction = Model.transform(validation)\n",
    "    tp = prediction.filter(\"label = 1 and prediction = 1\").count()\n",
    "    fp = prediction.filter(\"label = 0 and prediction = 1\").count()\n",
    "    fn = prediction.filter(\"label = 1 and prediction = 0\").count()\n",
    "    precision = tp / (tp + fp) \n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2*precision*recall / (precision+recall)\n",
    "    result = []\n",
    "    result.append(precision)\n",
    "    result.append(recall)\n",
    "    result.append(f1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the reulst for LR model\n",
    "result=[]\n",
    "result = getF1(Model_lr,validation)\n",
    "print(\"precision is {}\".format(result[0]))\n",
    "print(\"recall is {}\".format(result[1]))\n",
    "print(\"F1 Score is {}\".format(result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the reulst for gbt model\n",
    "result=[]\n",
    "result = getF1(Model_gbt,validation)\n",
    "print(\"precision is {}\".format(result[0]))\n",
    "print(\"recall is {}\".format(result[1]))\n",
    "print(\"F1 Score is {}\".format(result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the reulst for dt model\n",
    "result=[]\n",
    "result = getF1(Model_dt,validation)\n",
    "print(\"precision is {}\".format(result[0]))\n",
    "print(\"recall is {}\".format(result[1]))\n",
    "print(\"F1 Score is {}\".format(result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use undersampling to optimize the F1 score\n",
    "new_train = train.sampleBy('label', fractions={0: 99/349, 1: 1.0}).cache()\n",
    "new_train.groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use refined train data set to train the model again\n",
    "lrs =  LogisticRegression()\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lrs.elasticNetParam,[0.0, 0.1, 0.5, 1.0]) \\\n",
    "    .addGrid(lrs.regParam,[0.0, 0.05, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "crossval_lrs = CrossValidator(estimator=lrs,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "Model_lrs = crossval_lrs.fit(new_train)\n",
    "Model_lrs.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use refined train data set to train the model again\n",
    "dts = DecisionTreeClassifier()\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dts.impurity,['entropy', 'gini']) \\\n",
    "    .addGrid(dts.maxDepth,[2, 3, 4, 5, 6, 7, 8]) \\\n",
    "    .build()\n",
    "crossval_dts = CrossValidator(estimator=dts,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "Model_dts = crossval_dts.fit(new_train)\n",
    "Model_dts.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use refined train data set to train the model again\n",
    "gbts = GBTClassifier()\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbts.maxIter,[3, 10, 20]) \\\n",
    "    .addGrid(gbts.maxDepth,[2, 4, 6, 8]) \\\n",
    "    .build()\n",
    "crossval_gbts = CrossValidator(estimator=gbts,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "Model_gbts = crossval_gbts.fit(new_train)\n",
    "Model_gbts.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the reulst for LRS model\n",
    "result=[]\n",
    "result = getF1(cvModel_lrs,validation)\n",
    "print(\"precision is {}\".format(result[0]))\n",
    "print(\"recall is {}\".format(result[1]))\n",
    "print(\"F1 Score is {}\".format(result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the reulst for DTS model\n",
    "result=[]\n",
    "result = getF1(cvModel_dts,validation)\n",
    "print(\"precision is {}\".format(result[0]))\n",
    "print(\"recall is {}\".format(result[1]))\n",
    "print(\"F1 Score is {}\".format(result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the reulst for gbts model\n",
    "result=[]\n",
    "result = getF1(Model_gbts,validation)\n",
    "print(\"precision is {}\".format(result[0]))\n",
    "print(\"recall is {}\".format(result[1]))\n",
    "print(\"F1 Score is {}\".format(result[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 最后一步\n",
    "清理你的代码，添加注释和重命名变量，使得代码更易读和易于维护。参考 Spark 项目概述页面和数据科学家毕业项目审阅要求，确保你的项目包含了毕业项目要求的所有内容，并且满足所有审阅要求。记得在 GitHub 代码库里包含一份全面的文档——README文件，以及一个网络应用程序或博客文章。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda66023917b8b940ba965420f34affb640"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
